# Transcriptor de Charlas para multiples agentes. 

游꿏勇끁Probar en Google Colab <img src="https://camo.githubusercontent.com/f5e0d0538a9c2972b5d413e0ace04cecd8efd828d133133933dfffec282a4e1b/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" />](https://colab.research.google.com/github/SetHet/TranscriptorCharlas/blob/main/TranscriptorCharlas_v10.ipynb)

Este proyecto se basa en este Google Colab [transcripts with speaker names](https://colab.research.google.com/drive/1V-Bt5Hm2kjaDb4P1RyMSswsDKyrzc2-3?usp=sharing#scrollTo=O0_tup8RAyBy). Realize mejoras para mejorar la precision y generar archivos de texto. Divid칤 el archivo entrante de audio y lo procese en fragmentos para corregir un error de transcripci칩n que afectaba a audios demasiado extensos.

Este proyecto esta dise침ado para ser ejecutado en Google Colab con sus ambientes de GPU.

### Notas de uso

- Utilizar el modo GPU de Google Colab, Recomendado T4 GPU. Esto se encuentra en "Entorno de ejecuci칩n/Cambiar tipo de entorno de ejecuci칩n/T4 GPU" (https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm).
- La transcripci칩n se guardara en formato TXT y HTML y se descargaran autom치ticamente, puede que el explorador pida permiso de multi descargas.
- Cambie el numero de hablantes seg칰n el audio entregado.
- Seleccione un modelo m치s grande si tu quieres m치s precision y un modelo menor si tu quieres que se ejecute r치pidamente. Para T4_GPU se puede usar el modelo de mayor tama침o ([m치s informaci칩n](https://github.com/openai/whisper#available-models-and-languages)).
- Si el lenguaje es ingles, selecciona el idioma, sino el general funciona bastante bien, especialmente en espa침ol.

### Vista de alto nivel de lo que ocurre en el c칩digo

1.  Se sube un audio por el sistema de Google Colab, este proceso puede tardar bastante seg칰n el tama침o del archivo.
2.  Se instalaran los paquetes necesarios para el funcionamiento y cargara el modelo en memoria de video.
3.  Se separara el audio en trozos que se procesaran individualmente para mejorar la precision de la transcripci칩n. Esto se realiza con PyDub.
4.  Se procesaran lada trozo en varios segmentos de audio y generaran las transcripciones. Esto se realiza con Whisper de OpenAI.
5.  Se reagruparan los trozos de audio y texto, para su procesamiento de detecci칩n de hablantes. Esto se realiza mediante CLusters de aglomeraci칩n.
6.  Se generara un archivo de texto y otro de html para una mejor lectura.

<hr>

Espero que te sea util. Dime si esto puede ser mejorado 仇벒잺.
