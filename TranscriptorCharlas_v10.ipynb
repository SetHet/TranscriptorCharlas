{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SetHet/TranscriptorCharlas/blob/main/TranscriptorCharlas_v10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transcriptor de Charlas para multiples agentes.\n",
        "\n",
        "Este proyecto se basa en este Google Colab [transcripts with speaker names](https://colab.research.google.com/drive/1V-Bt5Hm2kjaDb4P1RyMSswsDKyrzc2-3?usp=sharing#scrollTo=O0_tup8RAyBy). Realize mejoras para mejorar la precision y generar archivos de texto. Dividi el archivo entrante de audio y lo procese en fragmentos para corregir un error de transcripcion que afectaba a audios demaciado extensos.\n",
        "\n",
        "Este proyecto esta diseñado para ser ejecutado en Google Colab con sus ambientes de GPU."
      ],
      "metadata": {
        "id": "SvY1Z7rMVjmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notas de uso:\n",
        "\n",
        "- Utilizar el modo GPU de Google Colab, Recomendado T4 GPU. Esto se encuentra en \"Entorno de ejecución/Cambiar tipo de entorno de ejecucion/T4 GPU\" (https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm).\n",
        "- La transcripcion se guardara en formato TXT y HTML y se descargaran automaticamente, puede que el explorador pida permiso de multi descargas.\n",
        "- Cambie el numero de hablantes segun el audio entregado.\n",
        "- Seleccione un modelo más grande si tu quieres más precision y un modelo menor si tu quieres que se ejecute rapidamente. Para T4_GPU se puede usar el modelo de mayor tamaño ([más información](https://github.com/openai/whisper#available-models-and-languages)).\n",
        "- Si el lenguaje es ingles, selecciona el idioma, sino el general funciona bastante bien, especialmente en español.\n",
        "\n",
        "\n",
        "Vista de alto nivel de lo que ocurre en el codigo:\n",
        "\n",
        "\n",
        "1.   Se sube un audio por el sistema de google colab, este proceso puede tardar bastante segun el tamaño del archivo.\n",
        "2.   Se instalaran los paquetes necesarios para el funcionamiento y cargara el modelo en memoria de video.\n",
        "3.   Se separara el audio en trozos que se procesaran individualmente para mejorar la precision de la transcripcion. Esto se realiza con PyDub.\n",
        "4.   Se procesaran lada trozo en varios segmentos de audio y generaran las transcripciones. Esto se realiza con Whisper de OpenAI.\n",
        "5.   Se reagruparan los trozos de audio y texto, para su procesamiento de deteccion de hablantes. Esto se realiza mediante CLusters de aglomeración.\n",
        "6.   Se generara un archivo de texto y otro de html para una mejor lectura.\n",
        "\n",
        "Espero que te sea util. Dime si esto puede ser mejorado.\n"
      ],
      "metadata": {
        "id": "4VKgaJB6RCmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes on usage:\n",
        "\n",
        "- Make sure to [change runtime to GPU](https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm).\n",
        "- The transcript will be saved in Files, which you can find in the menu on the left.\n",
        "- Change the number of speakers below if different from two.\n",
        "- Pick a bigger model if you want more accuracy and a smaller model if you want the program to run faster ([more info](https://github.com/openai/whisper#available-models-and-languages)).\n",
        "- If you know the language being spoken is English, then change language to 'English' as this improves performance.\n",
        "\n",
        "\n",
        "High level overview of what's happening here:\n",
        "\n",
        "\n",
        "1.   I'm using Open AI's Whisper model to seperate audio into segments and generate transcripts.\n",
        "2.   I'm then generating speaker embeddings for each segments.\n",
        "3.   Then I'm using agglomerative clustering on the embeddings to identify the speaker for each segment.\n",
        "\n",
        "Let me know if I can make it better!\n"
      ],
      "metadata": {
        "id": "ACobbJnIR_ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuracion"
      ],
      "metadata": {
        "id": "mAX_eZSeYli4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"audio.wav\"\n",
        "version_tscript = 10 #@param {type: \"integer\"}\n",
        "\n",
        "num_speakers = 2 #@param {type:\"integer\"}\n",
        "\n",
        "language = 'any' #@param ['any', 'English', 'es']\n",
        "\n",
        "model_size = 'large-v2' #@param ['tiny', 'base', 'small', 'medium', 'large', 'large-v2']\n",
        "\n",
        "clips_time_min = 10 #@param {type: \"integer\"}\n",
        "direction_clips = 'clips/' #@param {type:\"string\"}\n",
        "\n",
        "direction_text = 'texts/' #@param {type:\"string\"}\n",
        "\n",
        "model_name = model_size\n",
        "if language == 'English' and model_size != 'large':\n",
        "  model_name += '.en'\n"
      ],
      "metadata": {
        "id": "vGBTkJ14-Hfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargar Audio\n",
        "Al ejecutarse esta sección hay que cargar el audio"
      ],
      "metadata": {
        "id": "G6YmeCmqYp2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload audio file\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "path = next(iter(uploaded))"
      ],
      "metadata": {
        "id": "IxYITgmGfR9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalacion de bibliotecas"
      ],
      "metadata": {
        "id": "KM7mrJP9YetV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchvision torchaudio torchdata torchtext triton -y\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 torchtext==0.15.2  triton==2.0.0 torchdata==0.6.1\n",
        "!pip install openai cohere tiktoken\n",
        "!pip install speechbrain==0.5.16\n",
        "print(\"p1\")\n",
        "#!pip install -q git+https://github.com/openai/whisper.git@v20230918 > /dev/null\n",
        "!pip install -q git+https://github.com/openai/whisper.git > /dev/null\n",
        "print(\"p2\")\n",
        "#!pip install -q git+https://github.com/pyannote/pyannote-audio@b7960890aefda70e9595609d52e61e7e61fb16d7 > /dev/null\n",
        "!pip install -q git+https://github.com/pyannote/pyannote-audio > /dev/null\n",
        "print(\"p3\")\n",
        "!pip install PyDub\n",
        "!pip install lightning-fabric"
      ],
      "metadata": {
        "id": "34rLktlM6Or8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importaciones"
      ],
      "metadata": {
        "id": "A-UYhvobY26Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0_tup8RAyBy"
      },
      "outputs": [],
      "source": [
        "\n",
        "nombre_archivo = path\n",
        "\n",
        "import whisper\n",
        "import datetime\n",
        "\n",
        "import subprocess\n",
        "\n",
        "import torch\n",
        "import pyannote.audio\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
        "embedding_model = PretrainedSpeakerEmbedding(\n",
        "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "    device=torch.device(\"cuda\"))\n",
        "\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargar modelo Whisper"
      ],
      "metadata": {
        "id": "O73fLgv0ZLYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(model_size)"
      ],
      "metadata": {
        "id": "GibHaw1MxtbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subdividir Audio\n",
        "Cortar audio en trozos medianos para mejorar el procesamiento."
      ],
      "metadata": {
        "id": "PKDu50ZBY68g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm clips/*\n",
        "!rm clips\n",
        "!mkdir clips\n",
        "!rm texts/*\n",
        "!rm texts\n",
        "!mkdir texts\n",
        "\n",
        "#import wave\n",
        "#import contextlib\n",
        "\n",
        "if path[-3:] != 'wav':\n",
        "  subprocess.call(['ffmpeg', '-i', path, 'audio.wav', '-y'])\n",
        "  path = 'audio.wav'\n",
        "\n",
        "with contextlib.closing(wave.open(path,'r')) as f:\n",
        "    frames = f.getnframes()\n",
        "    rate = f.getframerate()\n",
        "    duration = frames / float(rate)\n",
        "    duration_total = duration\n",
        "\n",
        "#!pip install pydub\n",
        "\n",
        "from pydub import AudioSegment\n",
        "\n",
        "song = AudioSegment.from_file(\"audio.wav\", format=\"wav\")\n",
        "#with AudioSegment.from_file(\"audio.wav\", format=\"wav\") as song:\n",
        "# PyDub handles time in milliseconds\n",
        "seg = 1000\n",
        "\n",
        "clips = []\n",
        "while len(clips) * clips_time_min * 60 < duration:\n",
        "  len_clips = len(clips)\n",
        "  clips.append({\n",
        "      \"start\": len_clips * clips_time_min * 60 * seg,\n",
        "      \"end\": min((len_clips+1) * clips_time_min * 60 * seg, duration * seg),\n",
        "      \"file-clip\": direction_clips+\"clip-\"+str(len_clips)+\".wav\",\n",
        "      \"file-txt\": direction_text+\"part-\"+str(len_clips)+\".txt\"\n",
        "  })\n",
        "\n",
        "\n",
        "for i, clip in enumerate(clips):\n",
        "\n",
        "  print(\"init cut clip \" + str(i))\n",
        "  print(clip)\n",
        "  start = clip[\"start\"]\n",
        "  end = clip[\"end\"]\n",
        "  name_clip = clip[\"file-clip\"]\n",
        "  clip_sound = song[start:end]\n",
        "  #with song[start:end] as clip_sound:\n",
        "  clip_sound.export(name_clip, format=\"wav\")\n",
        "  print(name_clip)\n",
        "\n",
        "song = None\n",
        "clip_sound = None"
      ],
      "metadata": {
        "id": "DiE3hs3jnTlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deteccion\n",
        "1. Se detecta el texto hablado en los audios\n",
        "2. Se agrupan los textos y audios\n",
        "3. Se realiza una Clusterización para identificar los posibles hablantes\n",
        "4. Se ejecuta el cluster en cada segmento de audio para detectar al hablante"
      ],
      "metadata": {
        "id": "bfOZsqshZZ-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_embedding(segment):\n",
        "  start = segment[\"start\"]\n",
        "  # Whisper overshoots the end timestamp in the last segment\n",
        "  end = min(duration, segment[\"end\"])\n",
        "  clip = Segment(start, end)\n",
        "  waveform, sample_rate = audio.crop(path, clip)\n",
        "  return embedding_model(waveform[None])\n",
        "\n",
        "def time(secs):\n",
        "  return datetime.timedelta(seconds=round(secs))\n",
        "\n",
        "\n",
        "\n",
        "segments_group = []\n",
        "segment_embedding_group = []\n",
        "for y, clip in enumerate(clips):\n",
        "  print(\"Clip \"+ str(y)+ \" - \" + str(y+1)+\"/\"+str(len(clips)) + \" workinggg....\")\n",
        "  path = clip[\"file-clip\"]\n",
        "\n",
        "  result = model.transcribe(path)\n",
        "  segments = result[\"segments\"]\n",
        "\n",
        "  with contextlib.closing(wave.open(path,'r')) as f:\n",
        "    frames = f.getnframes()\n",
        "    rate = f.getframerate()\n",
        "    duration = frames / float(rate)\n",
        "  audio = Audio()\n",
        "  for i, segment in enumerate(segments):\n",
        "    segment_embedding_group.append(segment_embedding(segment))\n",
        "  for i in range(len(segments)):\n",
        "    segments[i][\"start\"] += clips_time_min*y*60\n",
        "    segments[i][\"end\"] += clips_time_min*y*60\n",
        "  segments_group += segments\n",
        "\n",
        "\n",
        "embeddings = np.zeros(shape=(len(segments_group), 192))\n",
        "for i, segment in enumerate(segments_group):\n",
        "  embeddings[i] = segment_embedding_group[i]\n",
        "\n",
        "embeddings = np.nan_to_num(embeddings)\n",
        "clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
        "labels = clustering.labels_\n",
        "for i in range(len(segments_group)):\n",
        "  segments_group[i][\"speaker\"] = 'SPEAKER' + str(labels[i] + 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vdbad9x5CHkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generación de Texto y HTML"
      ],
      "metadata": {
        "id": "m1JZPTFPZ6dg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generator"
      ],
      "metadata": {
        "id": "u1RgjWCPeIMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft = open(direction_text+\"translation.txt\", \"w\", encoding='utf-8')\n",
        "ft.write('Atencion: Documento transcrito por secciones e IA, Revisar\\n')\n",
        "ft.write('Datos:\\n')\n",
        "ft.write(f'- Version Transcriptior: v{version_tscript}\\n')\n",
        "ft.write('- Nombre archivo original: '+ nombre_archivo +'\\n')\n",
        "ft.write('- Duracion de la grabacion: '+ str(time(duration_total)) +'\\n')\n",
        "ft.write('- Duracion maxima de cada clip: '+ str(time(clips_time_min * 60)) +'\\n')\n",
        "ft.write('- Tamaño del modelo de Whisper: '+ model_size +'\\n')\n",
        "ft.write('- Idioma: '+ language +'\\n')\n",
        "ft.write('- Numero hablantes: '+ str(num_speakers) +'\\n')\n",
        "\n",
        "for (i, segment) in enumerate(segments_group):\n",
        "  if i == 0 or segments_group[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "    #f.write(\"\\n\" + segment[\"speaker\"] + ' ClipTime: ' + str(time(segment[\"start\"])))\n",
        "    ft.write(\"\\n\" + segment[\"speaker\"] + '    Time: ' + str(time(segment[\"start\"])) + '\\n')\n",
        "  ft.write(segment[\"text\"][1:] + ' ')\n",
        "\n",
        "ft.close()"
      ],
      "metadata": {
        "id": "Y6jxv2nGe5g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!zip -r outs.zip texts clips\n",
        "#!zip -r out_texts.zip texts\n",
        "files.download(direction_text+\"translation.txt\")"
      ],
      "metadata": {
        "id": "crshOY8GAX3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HTML Generator\n"
      ],
      "metadata": {
        "id": "Or5iwo7aeE9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft = open(direction_text+\"translation.html\", \"w\", encoding='utf-8')\n",
        "ft.write('<!DOCTYPE html><html lang=\"es\"><head> <meta charset=\"UTF-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"><title>Document</title></head><body>')\n",
        "ft.write('<h3>Atencion: Documento transcrito por secciones e IA, Revisar</h3>\\n')\n",
        "ft.write('<h3>Datos:</h3>\\n')\n",
        "ft.write('<ul>\\n')\n",
        "ft.write(f'<li>Version Transcriptior: v{version_tscript}</li>\\n')\n",
        "ft.write('<li>Nombre archivo original: '+ nombre_archivo +'</li>\\n')\n",
        "ft.write('<li>Duracion de la grabacion: '+ str(time(duration_total)) +'</li>\\n')\n",
        "ft.write('<li>Duracion maxima de cada clip: '+ str(time(clips_time_min * 60)) +'</li>\\n')\n",
        "ft.write('<li>Tamaño del modelo de Whisper: '+ model_size +'</li>\\n')\n",
        "ft.write('<li>Idioma: '+ language +'</li>\\n')\n",
        "ft.write('<li>Numero hablantes: '+ str(num_speakers) +'</li>\\n')\n",
        "ft.write('</ul><p>')\n",
        "\n",
        "for (i, segment) in enumerate(segments_group):\n",
        "  if i == 0 or segments_group[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "    ft.write(\"\\n</p><p>\" + segment[\"speaker\"] + '    Time: ' + str(time(segment[\"start\"])) + '</p>\\n<p>')\n",
        "  ft.write('<span style=\"color: orange;\">[SP.{}={}-{}]</span> '.format(segment[\"speaker\"][-1], str(time(segment[\"start\"])), str(time(segment[\"end\"])))+segment[\"text\"][1:] + ' ')\n",
        "ft.write('</p></body></html>')\n",
        "\n",
        "ft.close()"
      ],
      "metadata": {
        "id": "J2nUuePteCyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "files.download(direction_text+\"translation.html\")"
      ],
      "metadata": {
        "id": "cPLe6vk9eOPm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}